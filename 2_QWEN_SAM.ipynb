{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Qwen + SAM: IA Conversacional para Segmentación Inteligente\n",
        "\n",
        "Este notebook demuestra un enfoque **más avanzado y conversacional** para segmentación usando **Qwen2.5-VL**, un modelo multimodal que entiende tanto imágenes como texto natural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuración e Imports\n",
        "\n",
        "Importamos **Transformers** y **qwen-vl-utils**, que nos permite usar modelos de visión y lenguaje para conversaciones sobre imágenes de forma sencilla, gratuita y en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YauhUYCY8DBD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.6.0+cu124, using device: cuda\n",
            "transformers version: 4.52.4\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import qwen_vl_utils\n",
        "except ImportError:\n",
        "    !pip install qwen-vl-utils\n",
        "    import qwen_vl_utils\n",
        "import torch\n",
        "import transformers\n",
        "from huggingface_hub import snapshot_download\n",
        "from transformers import SamProcessor, SamModel\n",
        "from transformers import AutoProcessor\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"torch version: {torch.__version__}, using device: {device}\")\n",
        "print(f\"transformers version: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Descarga de Modelos en Local\n",
        "\n",
        "**Diferencia clave**: Solo necesitamos dos modelos para este enfoque más simplificado:\n",
        "\n",
        "- **Qwen2.5-VL-3B**: Modelo multimodal que entiende imágenes y texto como una conversación\n",
        "- **SAM**: Segmentación precisa (igual que en el notebook anterior)\n",
        "\n",
        "**Ventaja**: Qwen reemplaza múltiples modelos (DINO, CLIP, BLIP) con uno solo más potente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7f85fdf5b5549dd9a18e299993022c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cedfe33a4ff46aa8c7a502fca2b41cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d84ee2af5484bd5b177b3e6c0fd91cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    HF_MODELS_DIR = Path(\"/content/drive/MyDrive/hf_models\")\n",
        "else:\n",
        "    HF_MODELS_DIR = Path(r\"G:\\Mi unidad\\hf_models\")\n",
        "\n",
        "HF_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "models = [\n",
        "    \"facebook/sam-vit-large\",\n",
        "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "    \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
        "]\n",
        "\n",
        "for repo in models: #Atención! descargará unos 7 GB mas a Google Drive\n",
        "    local_dir = HF_MODELS_DIR / repo\n",
        "    snapshot_download(\n",
        "        repo_id=repo,\n",
        "        local_dir=str(local_dir),\n",
        "        local_dir_use_symlinks=False,\n",
        "        resume_download=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Carga de Modelos\n",
        "\n",
        "**Qwen2.5-VL** es un modelo **multimodal** que puede:\n",
        "- Responder preguntas sobre imágenes\n",
        "- Detectar objetos cuando se lo pedimos  \n",
        "- Generar bounding boxes en formato JSON\n",
        "- Mantener conversaciones sobre el contenido visual\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m qwen_path = HF_MODELS_DIR / \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-VL-7B-Instruct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m qwen_processor = AutoProcessor.from_pretrained(qwen_path)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m qwen_model = \u001b[43mQwen2_5_VLForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqwen_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflash_attention_2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\PHD\\prompt_to_mask\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\PHD\\prompt_to_mask\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4499\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4497\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m4499\u001b[39m     config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4501\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4506\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4507\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m   4508\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(config, *model_args, **model_kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\PHD\\prompt_to_mask\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:2183\u001b[39m, in \u001b[36mPreTrainedModel._autoset_attn_implementation\u001b[39m\u001b[34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[39m\n\u001b[32m   2180\u001b[39m     config._attn_implementation = \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2183\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2189\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflex_attention\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2191\u001b[39m     config = \u001b[38;5;28mcls\u001b[39m._check_and_enable_flex_attn(config, hard_check_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\PHD\\prompt_to_mask\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:2325\u001b[39m, in \u001b[36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[39m\u001b[34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[39m\n\u001b[32m   2323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[32m   2324\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2327\u001b[39m flash_attention_version = version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mflash_attn\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   2328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.version.cuda:\n",
            "\u001b[31mImportError\u001b[39m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
          ]
        }
      ],
      "source": [
        "# models\n",
        "\n",
        "# utilizar estos para no descargarlos en local\n",
        "# sam_path = \"facebook/sam-vit-large\"\n",
        "# qwen_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "# utilizar estos si están descargados en local\n",
        "sam_path = HF_MODELS_DIR / \"facebook/sam-vit-large\"\n",
        "qwen_path = HF_MODELS_DIR / \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "sam_processor = SamProcessor.from_pretrained(sam_path)\n",
        "sam_model = SamModel.from_pretrained(sam_path).to(device)\n",
        "\n",
        "qwen_processor = AutoProcessor.from_pretrained(qwen_path)\n",
        "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(qwen_path, torch_dtype=torch.float16, device_map=\"auto\") # usamos half rpecision fp16 para que sea más rápido y ligero\n",
        "\n",
        "\n",
        "# qwen_path = HF_MODELS_DIR / \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
        "# qwen_processor = AutoProcessor.from_pretrained(qwen_path)\n",
        "# qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(qwen_path, torch_dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"flash_attention_2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Carga de Imágenes de Ejemplo\n",
        "\n",
        "Usamos las mismas imágenes que en el notebook anterior para poder comparar ambos enfoques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DyX6Bf-s8DBE"
      },
      "outputs": [],
      "source": [
        "# images\n",
        "url_cars = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/cars.jpg\"\n",
        "image_cars = Image.open(BytesIO(requests.get(url_cars).content)).convert(\"RGB\")\n",
        "\n",
        "url_person_cars = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/person_cars.jpg\"\n",
        "image_person_cars = Image.open(BytesIO(requests.get(url_person_cars).content)).convert(\"RGB\")\n",
        "\n",
        "url_fruits = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/fruits.jpg\"\n",
        "image_fruits = Image.open(BytesIO(requests.get(url_fruits).content)).convert(\"RGB\")\n",
        "\n",
        "url_park = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/park.jpg\"\n",
        "image_park = Image.open(BytesIO(requests.get(url_park).content)).convert(\"RGB\")\n",
        "\n",
        "url_carpet_ok = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/carpet_ok.jpg\"\n",
        "image_carpet_ok = Image.open(BytesIO(requests.get(url_carpet_ok).content)).convert(\"RGB\")\n",
        "\n",
        "url_carpet_nok = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/carpet_nok.jpg\"\n",
        "image_carpet_nok = Image.open(BytesIO(requests.get(url_carpet_nok).content)).convert(\"RGB\")\n",
        "\n",
        "url_board = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/board.jpg\"\n",
        "image_board = Image.open(BytesIO(requests.get(url_board).content)).convert(\"RGB\")\n",
        "\n",
        "url_cells = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/cells.jpg\"\n",
        "image_cells = Image.open(BytesIO(requests.get(url_cells).content)).convert(\"RGB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Qwen: Conversación sobre Imágenes\n",
        "\n",
        "**Demostración**: Qwen puede responder preguntas naturales sobre imágenes.\n",
        "\n",
        "**Proceso**:\n",
        "1. Creamos un mensaje tipo chat con imagen y pregunta\n",
        "2. `apply_chat_template` formatea la conversación\n",
        "3. `process_vision_info` extrae la información visual\n",
        "4. El modelo genera respuestas en lenguaje natural\n",
        "5. El modelo puede devolver respuestas estructuradas en JSON o texto libre según se solicite\n",
        "\n",
        "\n",
        "**Ventaja**: Mucho más flexible que modelos especializados - podemos hacer **cualquier pregunta**.\n",
        "\n",
        "**Desventaja**: Más pesado y lento que las otras alternativas.\n",
        "\n",
        "Model Card: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2502.13923\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'image_cells' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     42\u001b[39m     plt.show()\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m answer_question(\u001b[43mimage_cells\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mCount each one of all all the red blood cells in this microscope image and provide bounding box coordinates.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m answer_question(image_cells, \u001b[33m\"\u001b[39m\u001b[33mDetect each one of all the red blood cells in the image. For each cell, return the exact pixel coordinates as bounding boxes in JSON format.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'image_cells' is not defined"
          ]
        }
      ],
      "source": [
        "# qwen\n",
        "def answer_question(image, prompt):\n",
        "    \"\"\"Answer questions about images using Qwen model.\"\"\"\n",
        "    \n",
        "    # Crear estructura de conversación con imagen y texto\n",
        "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
        "\n",
        "    # Convertir mensajes al formato que espera el modelo\n",
        "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    \n",
        "    # Extraer y procesar información visual de los mensajes\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    \n",
        "    # Tokenizar texto, procesar imágenes y crear tensores finales\n",
        "    inputs = qwen_processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = qwen_model.generate(**inputs, max_new_tokens=256) #tamaño maximo de la respuesta\n",
        "        \n",
        "    # Decodificar solo la parte nueva generada\n",
        "    answer = qwen_processor.batch_decode([out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)], skip_special_tokens=True)[0]\n",
        "    print(f\"\\nPrompt: {prompt} \\nAnswer: {answer}\\n\")\n",
        "\n",
        "    img_array = np.array(image)\n",
        "    \n",
        "    # Buscar JSON en la respuesta\n",
        "    json_match = re.search(r'```json\\n(.*?)\\n```', answer, re.DOTALL)\n",
        "    if json_match:\n",
        "        data = json.loads(json_match.group(1))\n",
        "        \n",
        "        if data:\n",
        "            for item in data:\n",
        "                if 'bbox_2d' in item:\n",
        "                    x1, y1, x2, y2 = item['bbox_2d']\n",
        "                    label = item.get('label', 'object')\n",
        "                    \n",
        "                    cv2.rectangle(img_array, (x1, y1), (x2, y2), (0, 0, 255), 3)\n",
        "                    cv2.putText(img_array, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "            \n",
        "    plt.imshow(img_array)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "answer_question(image_cells, \"Count each one of all all the red blood cells in this microscope image and provide bounding box coordinates.\")\n",
        "\n",
        "answer_question(image_cells, \"Detect each one of all the red blood cells in the image. For each cell, return the exact pixel coordinates as bounding boxes in JSON format.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pipeline Avanzado: Qwen + SAM\n",
        "\n",
        "**Proceso**:\n",
        "1. **Qwen** entiende instrucciones complejas en lenguaje natural\n",
        "2. Genera bounding boxes en **formato JSON estructurado**\n",
        "3. **SAM** convierte las cajas en máscaras precisas\n",
        "\n",
        "**Ventajas sobre Grounding DINO**:\n",
        "- **Instrucciones más complejas**: \"Detecta coches pero no los amarillos\"\n",
        "- **Respuestas estructuradas**: JSON automático con etiquetas y coordenadas\n",
        "- **Mayor flexibilidad**: Un solo modelo para múltiples tareas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# qwen + sam\n",
        "\n",
        "def segment_objects(image, prompt):\n",
        "    \"\"\"Segment objects using Qwen + SAM pipeline\"\"\"\n",
        "    \n",
        "    boxes = []\n",
        "    labels = []\n",
        "    \n",
        "    if image is None:\n",
        "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
        "    else:\n",
        "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
        "\n",
        "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = qwen_processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = qwen_model.generate(**inputs, max_new_tokens=256)\n",
        "        \n",
        "    answer = qwen_processor.batch_decode([out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)], skip_special_tokens=True)[0]\n",
        "    print(f\"\\nPrompt: {prompt} \\nAnswer: {answer}\\n\")\n",
        "\n",
        "    if image is None:\n",
        "        return answer\n",
        "\n",
        "    json_match = re.search(r'```json\\n(.*?)\\n```', answer, re.DOTALL)\n",
        "    if json_match:\n",
        "        data = json.loads(json_match.group(1))\n",
        "        if data:\n",
        "            img_array = np.array(image)\n",
        "            for item in data:\n",
        "                if 'bbox_2d' in item:\n",
        "                    x1, y1, x2, y2 = item['bbox_2d']\n",
        "                    label = item.get('label', 'object')\n",
        "                    boxes.append([x1, y1, x2, y2])\n",
        "                    labels.append(label)\n",
        "\n",
        "    w, h = image.size\n",
        "    final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "    if len(boxes) > 0:\n",
        "        sam_inputs = sam_processor(images=image, input_boxes=[boxes], return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sam_outputs = sam_model(**sam_inputs)\n",
        "\n",
        "        masks = sam_processor.post_process_masks(sam_outputs.pred_masks, sam_inputs[\"original_sizes\"], sam_inputs[\"reshaped_input_sizes\"])\n",
        "        iou_scores = sam_outputs.iou_scores.cpu().numpy()\n",
        "        final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "        for i in range(len(boxes)):\n",
        "            detection_masks = masks[0][i].cpu().numpy()\n",
        "            best_idx = iou_scores[0][i].argmax()\n",
        "            final_mask += detection_masks[best_idx]\n",
        "\n",
        "        \n",
        "    final_mask = np.clip(final_mask, 0, 1)\n",
        "    final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) #Usamos filtro morfológico close para eliminar ruido\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis(\"off\")\n",
        "    axes[1].imshow(final_mask, cmap=\"gray\")\n",
        "    axes[1].set_title(\"Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "    masked_img = np.array(image) * final_mask[:,:,None]\n",
        "    axes[2].imshow(masked_img.astype(np.uint8))\n",
        "    axes[2].set_title(\"Masked Image\")\n",
        "    axes[2].axis(\"off\")\n",
        "    inverted_mask = 1 - final_mask\n",
        "    removed_img = np.array(image) * inverted_mask[:,:,None]\n",
        "    axes[3].imshow(removed_img.astype(np.uint8))\n",
        "    axes[3].set_title(\"Removed Mask\")\n",
        "    axes[3].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return final_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicio: Calcular perímetro\n",
        "\n",
        "Vamos a calcular el perímetro del lago de Central Park usando la imagen `image_park`. \n",
        "\n",
        "La solución consiste en:\n",
        "1. Segmentar la superficie de Central Park para obtener la escala\n",
        "2. Segmentar el lago dentro del parque\n",
        "3. Calcular el perímetro usando la escala obtenida\n",
        "\n",
        "\n",
        "Sustituye los `None`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Width of central park? \n",
            "Answer: The width of Central Park in New York City is approximately 2.5 miles (4 kilometers).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "park_mask = None # <- Segmentar la superficie de Central Park\n",
        "lake_mask = None # <- Segmentar el lago dentro del parque\n",
        "\n",
        "park_height = segment_objects(None, \"Width of central park?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m real_height_km = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# <- Altura real de central park\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m park_coords = np.column_stack(np.where(\u001b[43mpark_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m)) \u001b[38;5;66;03m#Convertimos a coordenadas x,y\u001b[39;00m\n\u001b[32m      4\u001b[39m y_min = park_coords[:, \u001b[32m0\u001b[39m].min()\n\u001b[32m      5\u001b[39m y_max = park_coords[:, \u001b[32m0\u001b[39m].max()\n",
            "\u001b[31mTypeError\u001b[39m: '>' not supported between instances of 'NoneType' and 'int'"
          ]
        }
      ],
      "source": [
        "\n",
        "real_height_km = None  # <- Altura real de central park\n",
        "\n",
        "park_coords = np.column_stack(np.where(park_mask > 0)) #Convertimos a coordenadas x,y\n",
        "y_min = park_coords[:, 0].min()\n",
        "y_max = park_coords[:, 0].max()\n",
        "\n",
        "#calculamos el peso pixel\n",
        "park_height_px = None # <- Altura de la superficie de Central Park en pixeles\n",
        "km_per_pixel = None # <- Peso de un pixel en km\n",
        "\n",
        "# Calculamos el contorno del blob del lago\n",
        "contours, _ = cv2.findContours(lake_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "perimeter_px = cv2.arcLength(max(contours, key=cv2.contourArea), True)\n",
        "\n",
        "# calculamos el perimetro en km\n",
        "perimeter_km = None # <- Perimetro del lago en km\n",
        "\n",
        "print(f\"Lake perimeter: {perimeter_km:.2f} km\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
