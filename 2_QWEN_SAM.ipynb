{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Qwen + SAM: IA Conversacional para Segmentación Inteligente\n",
        "\n",
        "Este notebook demuestra un enfoque **más avanzado y conversacional** para segmentación usando **Qwen2.5-VL**, un modelo multimodal que entiende tanto imágenes como texto natural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuración e Imports\n",
        "\n",
        "Importamos **Transformers** y **qwen-vl-utils**, que nos permite usar modelos de visión y lenguaje para conversaciones sobre imágenes de forma sencilla, gratuita y en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YauhUYCY8DBD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.6.0+cu124, using device: cuda\n",
            "transformers version: 4.52.4\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import qwen_vl_utils\n",
        "except ImportError:\n",
        "    !pip install qwen-vl-utils\n",
        "    import qwen_vl_utils\n",
        "import torch\n",
        "import transformers\n",
        "from huggingface_hub import snapshot_download\n",
        "from transformers import SamProcessor, SamModel\n",
        "from transformers import AutoProcessor\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"torch version: {torch.__version__}, using device: {device}\")\n",
        "print(f\"transformers version: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Descarga de Modelos en Local\n",
        "\n",
        "**Diferencia clave**: Solo necesitamos dos modelos para este enfoque más simplificado:\n",
        "\n",
        "- **Qwen2.5-VL-3B**: Modelo multimodal que entiende imágenes y texto como una conversación\n",
        "- **SAM**: Segmentación precisa (igual que en el notebook anterior)\n",
        "\n",
        "**Ventaja**: Qwen reemplaza múltiples modelos (DINO, CLIP, BLIP) con uno solo más potente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "583b077d9a504e03b90c837a172ca042",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1739e0bd8d794630b9f9d356ac2384e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    HF_MODELS_DIR = Path(\"/content/drive/MyDrive/hf_models\")\n",
        "else:\n",
        "    HF_MODELS_DIR = Path(r\"G:\\Mi unidad\\hf_models\")\n",
        "\n",
        "HF_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "models = [\n",
        "    \"facebook/sam-vit-large\",\n",
        "    \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "]\n",
        "\n",
        "for repo in models: #Atención! descargará unos 7 GB mas a Google Drive\n",
        "    local_dir = HF_MODELS_DIR / repo\n",
        "    snapshot_download(\n",
        "        repo_id=repo,\n",
        "        local_dir=str(local_dir),\n",
        "        local_dir_use_symlinks=False,\n",
        "        resume_download=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Carga de Modelos\n",
        "\n",
        "**Qwen2.5-VL** es un modelo **multimodal** que puede:\n",
        "- Responder preguntas sobre imágenes\n",
        "- Detectar objetos cuando se lo pedimos  \n",
        "- Generar bounding boxes en formato JSON\n",
        "- Mantener conversaciones sobre el contenido visual\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd88a2c362eb43098d607651242e5db2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# models\n",
        "\n",
        "# utilizar estos para no descargarlos en local\n",
        "# sam_path = \"facebook/sam-vit-large\"\n",
        "# qwen_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "# utilizar estos si están descargados en local\n",
        "sam_path = HF_MODELS_DIR / \"facebook/sam-vit-large\"\n",
        "qwen_path = HF_MODELS_DIR / \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "sam_processor = SamProcessor.from_pretrained(sam_path)\n",
        "sam_model = SamModel.from_pretrained(sam_path).to(device)\n",
        "\n",
        "qwen_processor = AutoProcessor.from_pretrained(qwen_path)\n",
        "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(qwen_path, torch_dtype=torch.float16, device_map=\"auto\") # usamos half rpecision fp16 para que sea más rápido y ligero\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Carga de Imágenes de Ejemplo\n",
        "\n",
        "Usamos las mismas imágenes que en el notebook anterior para poder comparar ambos enfoques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DyX6Bf-s8DBE"
      },
      "outputs": [],
      "source": [
        "# images\n",
        "url_cars = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/cars.jpg\"\n",
        "image_cars = Image.open(BytesIO(requests.get(url_cars).content)).convert(\"RGB\")\n",
        "\n",
        "url_person_cars = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/person_cars.jpg\"\n",
        "image_person_cars = Image.open(BytesIO(requests.get(url_person_cars).content)).convert(\"RGB\")\n",
        "\n",
        "url_fruits = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/fruits.jpg\"\n",
        "image_fruits = Image.open(BytesIO(requests.get(url_fruits).content)).convert(\"RGB\")\n",
        "\n",
        "url_park = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/park.jpg\"\n",
        "image_park = Image.open(BytesIO(requests.get(url_park).content)).convert(\"RGB\")\n",
        "\n",
        "url_carpet_ok = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/carpet_ok.jpg\"\n",
        "image_carpet_ok = Image.open(BytesIO(requests.get(url_carpet_ok).content)).convert(\"RGB\")\n",
        "\n",
        "url_carpet_nok = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/carpet_nok.jpg\"\n",
        "image_carpet_nok = Image.open(BytesIO(requests.get(url_carpet_nok).content)).convert(\"RGB\")\n",
        "\n",
        "url_board = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/board.jpg\"\n",
        "image_board = Image.open(BytesIO(requests.get(url_board).content)).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## **Qwen 2.5 VL**: Conversación sobre Imágenes\n",
        "\n",
        "Qwen2.5-VL es un modelo multimodal conversacional que combina visión y lenguaje.\n",
        "\n",
        "Puede analizar imágenes y responder preguntas en lenguaje natural o formato estructurado.\n",
        "\n",
        "- Modelo multimodal: procesa texto + imágenes simultáneamente\n",
        "- Arquitectura Transformer con Vision Encoder integrado\n",
        "- 3B parámetros: balance entre rendimiento y eficiencia\n",
        "- Entrenamiento conversacional: entiende instrucciones complejas\n",
        "- Salida estructurada: puede generar JSON\n",
        "- Zero-shot: funciona sin entrenamiento adicional\n",
        "\n",
        "\n",
        "**Proceso**:\n",
        "1. Creamos un mensaje tipo chat con imagen y pregunta\n",
        "2. `apply_chat_template` formatea la conversación\n",
        "3. `process_vision_info` extrae la información visual\n",
        "4. El modelo genera respuestas en lenguaje natural\n",
        "5. El modelo puede devolver respuestas estructuradas en JSON o texto libre según se solicite\n",
        "\n",
        "\n",
        "**Ventaja**: Mucho más flexible que modelos especializados - podemos hacer **cualquier pregunta**.\n",
        "\n",
        "**Desventaja**: Más pesado y lento que las otras alternativas.\n",
        "\n",
        "Model Card: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2502.13923\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# qwen\n",
        "def answer_question(image, prompt):\n",
        "    \"\"\"Answer questions about images using Qwen model.\"\"\"\n",
        "    \n",
        "    # Crear estructura de conversación con imagen y texto\n",
        "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
        "\n",
        "    # Convertir mensajes al formato que espera el modelo\n",
        "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    \n",
        "    # Extraer y procesar información visual de los mensajes\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    \n",
        "    # Tokenizar texto, procesar imágenes y crear tensores finales\n",
        "    inputs = qwen_processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = qwen_model.generate(**inputs, max_new_tokens=256) #tamaño maximo de la respuesta\n",
        "        \n",
        "    # Decodificar solo la parte nueva generada\n",
        "    answer = qwen_processor.batch_decode([out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)], skip_special_tokens=True)[0]\n",
        "    print(f\"\\nPrompt: {prompt} \\nAnswer: {answer}\\n\")\n",
        "\n",
        "    img_array = np.array(image)\n",
        "    \n",
        "    # Buscar JSON en la respuesta\n",
        "    json_match = re.search(r'```json\\n(.*?)\\n```', answer, re.DOTALL)\n",
        "    if json_match:\n",
        "        data = json.loads(json_match.group(1))\n",
        "        \n",
        "        if data:\n",
        "            for item in data:\n",
        "                if 'bbox_2d' in item:\n",
        "                    x1, y1, x2, y2 = item['bbox_2d']\n",
        "                    label = item.get('label', 'object')\n",
        "                    \n",
        "                    cv2.rectangle(img_array, (x1, y1), (x2, y2), (0, 0, 255), 3)\n",
        "                    cv2.putText(img_array, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "            \n",
        "    plt.imshow(img_array)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "answer_question(image_person_cars, \"What is the woman wearing?\") \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pipeline Avanzado: Qwen + SAM\n",
        "\n",
        "**Proceso**:\n",
        "1. **Qwen** entiende instrucciones complejas en lenguaje natural\n",
        "2. Genera bounding boxes en **formato JSON estructurado**\n",
        "3. **SAM** convierte las cajas en máscaras precisas\n",
        "\n",
        "**Ventajas sobre Grounding DINO**:\n",
        "- **Instrucciones más complejas**: \"Detecta coches pero no los amarillos\"\n",
        "- **Respuestas estructuradas**: JSON automático con etiquetas y coordenadas\n",
        "- **Mayor flexibilidad**: Un solo modelo para múltiples tareas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# qwen + sam\n",
        "\n",
        "def segment_objects(image, prompt):\n",
        "    \"\"\"Segment objects using Qwen + SAM pipeline\"\"\"\n",
        "    \n",
        "    boxes = []\n",
        "    labels = []\n",
        "    \n",
        "    if image is None:\n",
        "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
        "    else:\n",
        "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}]\n",
        "\n",
        "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = qwen_processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = qwen_model.generate(**inputs, max_new_tokens=256)\n",
        "        \n",
        "    answer = qwen_processor.batch_decode([out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)], skip_special_tokens=True)[0]\n",
        "    print(f\"\\nPrompt: {prompt} \\nAnswer: {answer}\\n\")\n",
        "\n",
        "    if image is None:\n",
        "        return answer\n",
        "\n",
        "    json_match = re.search(r'```json\\n(.*?)\\n```', answer, re.DOTALL)\n",
        "    if json_match:\n",
        "        data = json.loads(json_match.group(1))\n",
        "        if data:\n",
        "            img_array = np.array(image)\n",
        "            for item in data:\n",
        "                if 'bbox_2d' in item:\n",
        "                    x1, y1, x2, y2 = item['bbox_2d']\n",
        "                    label = item.get('label', 'object')\n",
        "                    boxes.append([x1, y1, x2, y2])\n",
        "                    labels.append(label)\n",
        "\n",
        "    w, h = image.size\n",
        "    final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "    if len(boxes) > 0:\n",
        "        sam_inputs = sam_processor(images=image, input_boxes=[boxes], return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sam_outputs = sam_model(**sam_inputs)\n",
        "\n",
        "        masks = sam_processor.post_process_masks(sam_outputs.pred_masks, sam_inputs[\"original_sizes\"], sam_inputs[\"reshaped_input_sizes\"])\n",
        "        iou_scores = sam_outputs.iou_scores.cpu().numpy()\n",
        "        final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "        for i in range(len(boxes)):\n",
        "            detection_masks = masks[0][i].cpu().numpy()\n",
        "            best_idx = iou_scores[0][i].argmax()\n",
        "            final_mask += detection_masks[best_idx]\n",
        "\n",
        "        \n",
        "    final_mask = np.clip(final_mask, 0, 1)\n",
        "    final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) #Usamos filtro morfológico close para eliminar ruido\n",
        "    final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis(\"off\")\n",
        "    axes[1].imshow(final_mask, cmap=\"gray\")\n",
        "    axes[1].set_title(\"Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "    masked_img = np.array(image) * final_mask[:,:,None]\n",
        "    axes[2].imshow(masked_img.astype(np.uint8))\n",
        "    axes[2].set_title(\"Masked Image\")\n",
        "    axes[2].axis(\"off\")\n",
        "    inverted_mask = 1 - final_mask\n",
        "    removed_img = np.array(image) * inverted_mask[:,:,None]\n",
        "    axes[3].imshow(removed_img.astype(np.uint8))\n",
        "    axes[3].set_title(\"Removed Mask\")\n",
        "    axes[3].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return final_mask\n",
        "\n",
        "\n",
        "mask = segment_objects(image_person_cars, \"Detect cars but not the yellow ones and provide bounding box coordinates.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicio: Calcular perímetro\n",
        "\n",
        "Vamos a calcular el perímetro del lago de Central Park usando la imagen `image_park`. \n",
        "\n",
        "La solución consiste en:\n",
        "1. Segmentar la superficie de Central Park para obtener la escala\n",
        "2. Segmentar el lago dentro del parque\n",
        "3. Calcular el perímetro usando la escala obtenida\n",
        "\n",
        "\n",
        "![Central Park](assets/park.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Sustituye los `None` por los valores correctos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Width of central park? \n",
            "Answer: The width of Central Park in New York City is approximately 2.5 miles (4 kilometers).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "park_mask = None # <- Segmentar la superficie de Central Park\n",
        "lake_mask = None # <- Segmentar la superficie del lago\n",
        "\n",
        "park_height = segment_objects(None, \"Width of central park?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m real_height_km = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# <- Altura real de central park\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m park_coords = np.column_stack(np.where(\u001b[43mpark_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m)) \u001b[38;5;66;03m#Convertimos a coordenadas x,y\u001b[39;00m\n\u001b[32m      4\u001b[39m y_min = park_coords[:, \u001b[32m0\u001b[39m].min()\n\u001b[32m      5\u001b[39m y_max = park_coords[:, \u001b[32m0\u001b[39m].max()\n",
            "\u001b[31mTypeError\u001b[39m: '>' not supported between instances of 'NoneType' and 'int'"
          ]
        }
      ],
      "source": [
        "\n",
        "real_height_km = None  # <- Altura real de central park\n",
        "\n",
        "park_coords = np.column_stack(np.where(park_mask > 0)) #Convertimos a coordenadas x,y\n",
        "y_min = park_coords[:, 0].min()\n",
        "y_max = park_coords[:, 0].max()\n",
        "\n",
        "#calculamos el peso pixel\n",
        "park_height_px = None # <- Altura de la superficie de Central Park en pixeles\n",
        "km_per_pixel = None # <- Peso de un pixel en km\n",
        "\n",
        "\n",
        "# Calculamos el contorno del blob del lago\n",
        "contours, _ = cv2.findContours(lake_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "perimeter_px = cv2.arcLength(max(contours, key=cv2.contourArea), True)\n",
        "\n",
        "# calculamos el perimetro en km\n",
        "perimeter_km = None # <- Perimetro del lago en km\n",
        "\n",
        "print(f\"Lake perimeter: {perimeter_km:.2f} km\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
