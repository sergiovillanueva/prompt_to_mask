{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Prompt to Mask: Combinando Modelos de IA para Segmentación Inteligente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuración e Imports\n",
        "\n",
        "Importamos las librerías principales, especialmente **Transformers de Hugging Face** que nos permite usar modelos preentrenados de forma sencilla, gratuita y en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YauhUYCY8DBD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.6.0+cu124, using device: cuda\n",
            "transformers version: 4.52.4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import SamProcessor, SamModel\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import transformers\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import warnings\n",
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "import sys\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"torch version: {torch.__version__}, using device: {device}\")\n",
        "print(f\"transformers version: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Descarga de Modelos en Local\n",
        "\n",
        "**Importante**: Descargamos todos los modelos a nuestro disco local para:\n",
        "- Evitar descargas repetidas\n",
        "- Trabajar sin conexión\n",
        "- Mejor rendimiento\n",
        "\n",
        "Los modelos que usaremos:\n",
        "- **GroundingDINO**: Detección de objetos usando texto\n",
        "- **SAM**: Segmentación precisa \n",
        "- **CLIP**: Clasificación imagen-texto\n",
        "- **BLIP**: Generación de descripciones\n",
        "\n",
        "\n",
        "\n",
        "Si no se dispone de espacio en disco suficiente se puede obviar esta celda y descargar el modelo en colab cada vez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "819be50b86ef40deba777bc7c093ca34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcac90c81a294b649d846b3bdb34e205",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "449301f544bd44aeb41fa44a47817b86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "562b02e44ece47b983ad3594e88180ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "if \"google.colab\" in sys.modules: #Detectamos si estamos en google colab o en local\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    HF_MODELS_DIR = Path(\"/content/drive/MyDrive/hf_models\")\n",
        "else:\n",
        "    HF_MODELS_DIR = Path(r\"G:\\Mi unidad\\hf_models\")\n",
        "\n",
        "HF_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "models = [\n",
        "    \"IDEA-Research/grounding-dino-base\",\n",
        "    \"facebook/sam-vit-large\",\n",
        "    \"openai/clip-vit-base-patch32\",\n",
        "    \"Salesforce/blip-image-captioning-base\"\n",
        "]\n",
        "\n",
        "for repo in models: #Atención! descargará unos 7 GB a Google Drive\n",
        "    local_dir = HF_MODELS_DIR / repo\n",
        "    snapshot_download(\n",
        "        repo_id=repo,\n",
        "        local_dir=str(local_dir),\n",
        "        local_dir_use_symlinks=False,\n",
        "        resume_download=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Carga de Modelos Pre-entrenados\n",
        "\n",
        "Cargamos los modelos desde las rutas locales usando **Transformers**. Nota cómo cada modelo tiene su processor específico que maneja el preprocesamiento de datos.\n",
        "\n",
        "Si no hemos descargado los modelos usar las rutas comentadas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# models\n",
        "\n",
        "# utilizar estos para no descargarlos en local\n",
        "# dino_path = \"IDEA-Research/grounding-dino-base\"\n",
        "# sam_path = \"facebook/sam-vit-large\"\n",
        "# clip_path = \"openai/clip-vit-base-patch32\"\n",
        "# blip_path = \"Salesforce/blip-image-captioning-base\"\n",
        "\n",
        "# utilizar estos si están descargados en local\n",
        "dino_path = HF_MODELS_DIR / \"IDEA-Research/grounding-dino-base\"\n",
        "sam_path = HF_MODELS_DIR / \"facebook/sam-vit-large\"\n",
        "clip_path = HF_MODELS_DIR / \"openai/clip-vit-base-patch32\"\n",
        "blip_path = HF_MODELS_DIR / \"Salesforce/blip-image-captioning-base\"\n",
        "\n",
        "dino_processor = AutoProcessor.from_pretrained(dino_path)\n",
        "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(dino_path).to(device)\n",
        "\n",
        "sam_processor = SamProcessor.from_pretrained(sam_path)\n",
        "sam_model = SamModel.from_pretrained(sam_path).to(device)\n",
        "\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_path)\n",
        "clip_model = CLIPModel.from_pretrained(clip_path).to(device)\n",
        "\n",
        "blip_processor = BlipProcessor.from_pretrained(blip_path)\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(blip_path, torch_dtype=torch.float16).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Carga de Imágenes de Ejemplo\n",
        "\n",
        "Descargamos imágenes de ejemplo desde el repositorio de GitHub para probar los diferentes modelos, podeis usar imagenes propias con: \n",
        "image_custom = Image.open(mi_path_custom).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DyX6Bf-s8DBE"
      },
      "outputs": [],
      "source": [
        "# images\n",
        "url_cars = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/cars.jpg\"\n",
        "image_cars = Image.open(BytesIO(requests.get(url_cars).content)).convert(\"RGB\")\n",
        "\n",
        "url_person_cars = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/person_cars.jpg\"\n",
        "image_person_cars = Image.open(BytesIO(requests.get(url_person_cars).content)).convert(\"RGB\")\n",
        "\n",
        "url_fruits = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/fruits.jpg\"\n",
        "image_fruits = Image.open(BytesIO(requests.get(url_fruits).content)).convert(\"RGB\")\n",
        "\n",
        "url_board = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/board.jpg\"\n",
        "image_board = Image.open(BytesIO(requests.get(url_board).content)).convert(\"RGB\")\n",
        "\n",
        "url_carpet_ok = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/carpet_ok.jpg\"\n",
        "image_carpet_ok = Image.open(BytesIO(requests.get(url_carpet_ok).content)).convert(\"RGB\")\n",
        "\n",
        "url_carpet_nok = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/carpet_nok.jpg\"\n",
        "image_carpet_nok = Image.open(BytesIO(requests.get(url_carpet_nok).content)).convert(\"RGB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## CLIP: Clasificación Imagen-Texto\n",
        "\n",
        "**CLIP** permite comparar una imagen con múltiples descripciones de texto y nos dice cuál es más probable. \n",
        "\n",
        "Útil para clasificación sin necesidad de entrenar en categorías específicas - usa **zero-shot learning**.\n",
        "\n",
        "Model Card: https://huggingface.co/openai/clip-vit-base-patch32\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2103.00020\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_image(image, descriptions):\n",
        "    inputs = clip_processor(text=descriptions, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    outputs = clip_model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "    for desc, prob in zip(descriptions, probs[0].tolist()):\n",
        "        print(f\"'{desc}' -> Prob: {prob:.2f}\")\n",
        "        \n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## BLIP: Generación de Descripciones\n",
        "\n",
        "**BLIP** genera automáticamente descripciones en texto a partir de imágenes.\n",
        "\n",
        "Proceso inverso a CLIP: en lugar de clasificar, **genera** texto descriptivo.\n",
        "\n",
        "Model Card: https://huggingface.co/Salesforce/blip-image-captioning-base\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2201.12086"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def caption_image(image):\n",
        "    inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    out = blip_model.generate(**inputs)\n",
        "    print(blip_processor.decode(out[0], skip_special_tokens=True))\n",
        "\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Grounding DINO: Detección Guiada por Texto\n",
        "\n",
        "**Grounding DINO** encuentra objetos en imágenes usando descripciones en lenguaje natural.\n",
        "\n",
        "A diferencia de detectores tradicionales que tienen categorías fijas, este modelo puede detectar **cualquier objeto** que describas con palabras.\n",
        "\n",
        "Devuelve cajas delimitadoras (bounding boxes) con coordenadas y puntuaciones de confianza.\n",
        "\n",
        "Model Card: https://huggingface.co/IDEA-Research/grounding-dino-base\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2303.05499"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S6WTOtOb8DBF"
      },
      "outputs": [],
      "source": [
        "def detect_objects(image, prompt):\n",
        "    w, h = image.size\n",
        "    scale = min(768 / max(h, w), 1.0)\n",
        "    new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "    # Aplicar Grounding DINO - procesamiento directo\n",
        "    inputs = dino_processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        do_resize=True,\n",
        "        size={\"height\": new_h, \"width\": new_w}\n",
        "    ).to(device)\n",
        "\n",
        "    # Ejecutar DINO para detección\n",
        "    with torch.no_grad():\n",
        "        outputs = dino_model(**inputs)\n",
        "\n",
        "    results = dino_processor.post_process_grounded_object_detection(\n",
        "                outputs,\n",
        "                inputs.input_ids,\n",
        "                threshold=0.7,\n",
        "                text_threshold=0.7,\n",
        "                target_sizes=[(h, w)])\n",
        "\n",
        "    boxes = results[0]['boxes']\n",
        "    labels = results[0]['labels']\n",
        "    scores = results[0]['scores']\n",
        "\n",
        "    # Mostrar resultados\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        print(f\"Box: {box}, Label: {label}, Score: {score}\")\n",
        "\n",
        "    img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        x0, y0, x1, y1 = map(int, box)\n",
        "        cv2.rectangle(img_cv, (x0, y0), (x1, y1), (0,0,255), 3)\n",
        "        txt = f\"{label} {score:.2f}\"\n",
        "        cv2.putText(img_cv, txt, (x0, y0-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,0,0), 2)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis(\"off\") \n",
        "    plt.show()\n",
        "    \n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## SAM: Segmentación Precisa\n",
        "\n",
        "**SAM (Segment Anything Model)** crea máscaras de segmentación precisas a nivel de píxel.\n",
        "\n",
        "Toma como entrada una caja delimitadora (del paso anterior) y genera una máscara que delimita exactamente el contorno del objeto.\n",
        "\n",
        "La máscara resultante es binaria: 1 para píxeles del objeto, 0 para fondo.\n",
        "\n",
        "Model Card: https://huggingface.co/facebook/sam-vit-large\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2304.02643"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FIoARLaL8DBF"
      },
      "outputs": [],
      "source": [
        "# sam\n",
        "\n",
        "def segment_box(image, box):\n",
        "    inputs = sam_processor(images=image, input_boxes=[[box]], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = sam_model(**inputs)\n",
        "\n",
        "    masks = sam_processor.post_process_masks(outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"])\n",
        "\n",
        "    best_mask = masks[0][0][outputs.iou_scores.argmax()].cpu().numpy() #cogermos la mejor de las máscaras por simplificar\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis(\"off\")\n",
        "    axes[1].imshow(best_mask, cmap=\"gray\")\n",
        "    axes[1].set_title(\"Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "    masked_img = np.array(image) * best_mask[:,:,None]\n",
        "    axes[2].imshow(masked_img.astype(np.uint8))\n",
        "    axes[2].set_title(\"Masked Image\")\n",
        "    axes[2].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show() \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pipeline Completo: Grounding DINO + SAM\n",
        "\n",
        "Combinamos ambos modelos para crear un sistema de segmentación guiado por texto.\n",
        "\n",
        "**Proceso**:\n",
        "1. **DINO** detecta objetos usando texto → obtiene cajas delimitadoras\n",
        "2. **SAM** refina cada caja → obtiene máscaras precisas  \n",
        "3. Combinamos todas las máscaras\n",
        "\n",
        "**Resultado**: Solo con texto natural podemos segmentar cualquier objeto en una imagen.\n",
        "\n",
        "**Modo Batch**: Procesa múltiples detecciones de una vez para mayor eficiencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nTnNlKaF8DBF"
      },
      "outputs": [],
      "source": [
        "# dino + sam\n",
        "\n",
        "def segment_objects(image, prompt, batch_mode=True):\n",
        "    \"\"\"Segment objects using DINO + SAM pipeline\"\"\"\n",
        "    w, h = image.size\n",
        "    scale = min(768 / max(h, w), 1.0)\n",
        "    new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "    inputs = dino_processor(images=image, text=prompt, return_tensors=\"pt\", do_resize=True, size={\"height\": new_h, \"width\": new_w}).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = dino_model(**inputs)\n",
        "\n",
        "    results = dino_processor.post_process_grounded_object_detection( outputs, inputs.input_ids, threshold=0.3, text_threshold=0.5, target_sizes=[(h, w)])\n",
        "\n",
        "    boxes = results[0]['boxes'].cpu().numpy().tolist()\n",
        "\n",
        "    final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "    if not batch_mode:\n",
        "        for box in boxes:\n",
        "            sam_inputs = sam_processor(images=image, input_boxes=[[box]], return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                sam_outputs = sam_model(**sam_inputs)\n",
        "\n",
        "            masks = sam_processor.post_process_masks(sam_outputs.pred_masks, sam_inputs[\"original_sizes\"], sam_inputs[\"reshaped_input_sizes\"])\n",
        "            best_mask = masks[0][0][sam_outputs.iou_scores.argmax()].cpu().numpy()\n",
        "            final_mask += best_mask\n",
        "\n",
        "    elif batch_mode:\n",
        "        if len(boxes) > 0:\n",
        "            sam_inputs = sam_processor(images=image, input_boxes=[boxes], return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                sam_outputs = sam_model(**sam_inputs)\n",
        "\n",
        "            masks = sam_processor.post_process_masks(sam_outputs.pred_masks, sam_inputs[\"original_sizes\"], sam_inputs[\"reshaped_input_sizes\"])\n",
        "            iou_scores = sam_outputs.iou_scores.cpu().numpy()\n",
        "            final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "            for i in range(len(boxes)):\n",
        "                detection_masks = masks[0][i].cpu().numpy()\n",
        "                best_idx = iou_scores[0][i].argmax()\n",
        "                final_mask += detection_masks[best_idx]\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    final_mask = np.clip(final_mask, 0, 1)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis(\"off\")\n",
        "    axes[1].imshow(final_mask, cmap=\"gray\")\n",
        "    axes[1].set_title(\"Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "    masked_img = np.array(image) * final_mask[:,:,None]\n",
        "    axes[2].imshow(masked_img.astype(np.uint8))\n",
        "    axes[2].set_title(\"Masked Image\")\n",
        "    axes[2].axis(\"off\")\n",
        "    inverted_mask = 1 - final_mask\n",
        "    removed_img = np.array(image) * inverted_mask[:,:,None]\n",
        "    axes[3].imshow(removed_img.astype(np.uint8))\n",
        "    axes[3].set_title(\"Removed Mask\")\n",
        "    axes[3].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
