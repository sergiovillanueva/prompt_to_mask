{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Prompt to Mask: Combinando Modelos de IA para Segmentación Inteligente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuración e Imports\n",
        "\n",
        "Importamos las librerías principales, especialmente **Transformers de Hugging Face** que nos permite usar modelos preentrenados de forma sencilla, gratuita y en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YauhUYCY8DBD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from huggingface_hub import snapshot_download\n",
        "from transformers import SamProcessor, SamModel\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"torch version: {torch.__version__}, using device: {device}\")\n",
        "print(f\"transformers version: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Descarga de Modelos en Local\n",
        "\n",
        "**Importante**: Descargamos todos los modelos a nuestro disco local para:\n",
        "- Evitar descargas repetidas\n",
        "- Trabajar sin conexión\n",
        "- Mejor rendimiento\n",
        "\n",
        "Los modelos que usaremos:\n",
        "- **GroundingDINO**: Detección de objetos usando texto\n",
        "- **SAM**: Segmentación precisa \n",
        "- **CLIP**: Clasificación imagen-texto\n",
        "- **BLIP**: Generación de descripciones\n",
        "\n",
        "\n",
        "\n",
        "Si no se dispone de espacio en disco suficiente se puede obviar esta celda y descargar el modelo en colab cada vez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if \"google.colab\" in sys.modules: #Detectamos si estamos en google colab o en local\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    HF_MODELS_DIR = Path(\"/content/drive/MyDrive/hf_models\")\n",
        "else:\n",
        "    HF_MODELS_DIR = Path(r\"G:\\Mi unidad\\hf_models\")\n",
        "\n",
        "HF_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "models = [\n",
        "    \"IDEA-Research/grounding-dino-base\",\n",
        "    \"facebook/sam-vit-large\",\n",
        "    \"openai/clip-vit-base-patch32\",\n",
        "    \"Salesforce/blip-image-captioning-base\"\n",
        "]\n",
        "\n",
        "for repo in models: #Atención! descargará unos 7 GB a Google Drive\n",
        "    local_dir = HF_MODELS_DIR / repo\n",
        "    snapshot_download(\n",
        "        repo_id=repo,\n",
        "        local_dir=str(local_dir),\n",
        "        local_dir_use_symlinks=False,\n",
        "        resume_download=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Carga de Modelos Pre-entrenados\n",
        "\n",
        "Cargamos los modelos desde las rutas locales usando **Transformers**. Nota cómo cada modelo tiene su processor específico que maneja el preprocesamiento de datos.\n",
        "\n",
        "Si no hemos descargado los modelos usar las rutas comentadas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# models\n",
        "\n",
        "# utilizar estos para no descargarlos en local\n",
        "# dino_path = \"IDEA-Research/grounding-dino-base\"\n",
        "# sam_path = \"facebook/sam-vit-large\"\n",
        "# clip_path = \"openai/clip-vit-base-patch32\"\n",
        "# blip_path = \"Salesforce/blip-image-captioning-base\"\n",
        "\n",
        "# utilizar estos si están descargados en local\n",
        "dino_path = HF_MODELS_DIR / \"IDEA-Research/grounding-dino-base\"\n",
        "sam_path = HF_MODELS_DIR / \"facebook/sam-vit-large\"\n",
        "clip_path = HF_MODELS_DIR / \"openai/clip-vit-base-patch32\"\n",
        "blip_path = HF_MODELS_DIR / \"Salesforce/blip-image-captioning-base\"\n",
        "\n",
        "dino_processor = AutoProcessor.from_pretrained(dino_path)\n",
        "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(dino_path).to(device)\n",
        "\n",
        "sam_processor = SamProcessor.from_pretrained(sam_path)\n",
        "sam_model = SamModel.from_pretrained(sam_path).to(device)\n",
        "\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_path)\n",
        "clip_model = CLIPModel.from_pretrained(clip_path).to(device)\n",
        "\n",
        "blip_processor = BlipProcessor.from_pretrained(blip_path)\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(blip_path, torch_dtype=torch.float16).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Carga de Imágenes de Ejemplo\n",
        "\n",
        "Descargamos imágenes de ejemplo desde el repositorio de GitHub para probar los diferentes modelos, podeis usar imagenes propias con: \n",
        "image_custom = Image.open(mi_path_custom).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyX6Bf-s8DBE"
      },
      "outputs": [],
      "source": [
        "# cargamos las imagenes del repositorio publico de github\n",
        "url_cars = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/cars.jpg\"\n",
        "image_cars = Image.open(BytesIO(requests.get(url_cars).content)).convert(\"RGB\")\n",
        "\n",
        "url_person_cars = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/person_cars.jpg\"\n",
        "image_person_cars = Image.open(BytesIO(requests.get(url_person_cars).content)).convert(\"RGB\")\n",
        "\n",
        "url_fruits = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/fruits.jpg\"\n",
        "image_fruits = Image.open(BytesIO(requests.get(url_fruits).content)).convert(\"RGB\")\n",
        "\n",
        "url_board = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/board.jpg\"\n",
        "image_board = Image.open(BytesIO(requests.get(url_board).content)).convert(\"RGB\")\n",
        "\n",
        "url_carpet_ok = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/carpet_ok.jpg\"\n",
        "image_carpet_ok = Image.open(BytesIO(requests.get(url_carpet_ok).content)).convert(\"RGB\")\n",
        "\n",
        "url_carpet_nok = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/carpet_nok.jpg\"\n",
        "image_carpet_nok = Image.open(BytesIO(requests.get(url_carpet_nok).content)).convert(\"RGB\")\n",
        "\n",
        "url_bananas = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/bananas.jpg\"\n",
        "image_bananas = Image.open(BytesIO(requests.get(url_bananas).content)).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## CLIP: Clasificación Imagen-Texto\n",
        "\n",
        "**CLIP** permite comparar una imagen con múltiples descripciones de texto y nos dice cuál es más probable. \n",
        "\n",
        "- Clasificación entrenar categorías específicas - usa **zero-shot learning**.\n",
        "\n",
        "- Entrenado con millones de pares imagen-texto de internet\n",
        "\n",
        "- Aprende representaciones visuales y textuales\n",
        "\n",
        "\n",
        "Model Card: https://huggingface.co/openai/clip-vit-base-patch32\n",
        "Paper: https://arxiv.org/pdf/2103.00020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_image(image, descriptions):\n",
        "    inputs = clip_processor(text=descriptions, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        \n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "    for desc, prob in zip(descriptions, probs[0].tolist()):\n",
        "        print(f\"'{desc}' -> Prob: {prob:.2f}\")\n",
        "        \n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "classify_image(image_person_cars, [\"a street with a person\", \"a dog\", \"several cars without any person\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## BLIP: Generación de Descripciones\n",
        "\n",
        "**BLIP** genera automáticamente descripciones en texto a partir de imágenes.\n",
        "\n",
        "- Proceso inverso a CLIP: en lugar de clasificar, **genera** texto descriptivo.\n",
        "- Entrenado con millones de pares imagen-texto\n",
        "- Genera descripciones automáticas y contextuales\n",
        "\n",
        "\n",
        "Model Card: https://huggingface.co/Salesforce/blip-image-captioning-base\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2201.12086"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def caption_image(image):\n",
        "    inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    with torch.no_grad():\n",
        "        out = blip_model.generate(**inputs)\n",
        "    print(blip_processor.decode(out[0], skip_special_tokens=True))\n",
        "\n",
        "    # mostrar la imagen\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    \n",
        "caption_image(image_person_cars) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Grounding DINO: Detección Guiada por Texto\n",
        "\n",
        "**Grounding DINO** encuentra objetos en imágenes usando descripciones en lenguaje natural.\n",
        "\n",
        "A diferencia de detectores tradicionales que tienen categorías fijas, puede detectar **cualquier objeto** que describas y devuelve bounding boxes con descripción y confianza.\n",
        "\n",
        "\n",
        "- Entrada: imagen + prompt en lenguaje natural\n",
        "- Salida: bounding boxes con coordenadas y scores de confianza\n",
        "- Thresholds configurables para detección y matching de texto\n",
        "- Usa CLIP para codificar imagen y texto en embeddings compartidos\n",
        "- Aplica transformer decoder para localizar objetos mencionados\n",
        "- Genera bounding boxes mediante regresión de coordenadas\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Model Card: https://huggingface.co/IDEA-Research/grounding-dino-base\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2303.05499"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6WTOtOb8DBF"
      },
      "outputs": [],
      "source": [
        "def detect_objects(image, prompt):\n",
        "    w, h = image.size\n",
        "    scale = min(768 / max(h, w), 1.0) #Entrada del modelo pero manteniendo la relacion de aspecto\n",
        "    new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "    # Aplicar Grounding DINO - procesamiento directo\n",
        "    inputs = dino_processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        do_resize=True,\n",
        "        size={\"height\": new_h, \"width\": new_w}\n",
        "    ).to(device)\n",
        "\n",
        "    # Ejecutar DINO para detección\n",
        "    with torch.no_grad():\n",
        "        outputs = dino_model(**inputs)\n",
        "\n",
        "    results = dino_processor.post_process_grounded_object_detection(\n",
        "                outputs,\n",
        "                inputs.input_ids,\n",
        "                threshold=0.6, #threshold de la detección (+ alto + confianza en la detección)\n",
        "                text_threshold=0.7, #threshold del texto (+ alto + estricto con el prompt)\n",
        "                target_sizes=[(h, w)])\n",
        "\n",
        "    boxes = results[0]['boxes']\n",
        "    labels = results[0]['labels']\n",
        "    scores = results[0]['scores']\n",
        "\n",
        "    # Mostrar resultados\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        print(f\"Box: {box}, Label: {label}, Score: {score}\")\n",
        "\n",
        "    img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        x0, y0, x1, y1 = map(int, box)\n",
        "        cv2.rectangle(img_cv, (x0, y0), (x1, y1), (0,0,255), 3)\n",
        "        txt = f\"{label} {score:.2f}\"\n",
        "        cv2.putText(img_cv, txt, (x0, y0-5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,0,0), 2)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis(\"off\") \n",
        "    plt.show()\n",
        "    \n",
        "    return\n",
        "\n",
        "detect_objects(image_fruits, [\"kiwi\", \"apple\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## SAM: Segmentación Precisa\n",
        "\n",
        "**SAM (Segment Anything Model)** crea máscaras de segmentación precisas a nivel de píxel sin entrenamiento.\n",
        "\n",
        "Toma como entrada una bounding box o unos puntos y genera varias máscaras que delimitan exactamente el contorno del objeto.\n",
        "\n",
        "\n",
        "- Entrenado con más de 1 billón de máscaras en 11 millones de imágenes\n",
        "- Utiliza arquitectura Vision Transformer (ViT) como backbone\n",
        "- Genera múltiples máscaras candidatas y selecciona la mejor automáticamente\n",
        "- La máscara resultante es binaria: 1 para píxeles del objeto, 0 para fondo.\n",
        "\n",
        "\n",
        "Model Card: https://huggingface.co/facebook/sam-vit-large\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2304.02643"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIoARLaL8DBF"
      },
      "outputs": [],
      "source": [
        "# sam\n",
        "\n",
        "def segment_box(image, box):\n",
        "    inputs = sam_processor(images=image, input_boxes=[[box]], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = sam_model(**inputs)\n",
        "\n",
        "    masks = sam_processor.post_process_masks(outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"])\n",
        "\n",
        "    best_mask = masks[0][0][outputs.iou_scores.argmax()].cpu().numpy() #cogermos la mejor de las máscaras por simplificar\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis(\"off\")\n",
        "    axes[1].imshow(best_mask, cmap=\"gray\")\n",
        "    axes[1].set_title(\"Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "    masked_img = np.array(image) * best_mask[:,:,None]\n",
        "    axes[2].imshow(masked_img.astype(np.uint8))\n",
        "    axes[2].set_title(\"Masked Image\")\n",
        "    axes[2].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show() \n",
        "    \n",
        "box = [] \n",
        "segment_box(image_person_cars, box)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pipeline Completo: Grounding DINO + SAM\n",
        "\n",
        "Combinamos ambos modelos para crear un sistema de segmentación guiado por texto.\n",
        "\n",
        "**Proceso**:\n",
        "1. **DINO** detecta objetos usando texto → obtiene cajas delimitadoras\n",
        "2. **SAM** refina cada caja → obtiene máscaras precisas  \n",
        "3. Combinamos todas las máscaras\n",
        "\n",
        "**Resultado**: Solo con texto natural podemos segmentar cualquier objeto en una imagen.\n",
        "\n",
        "**Modo Batch**: Procesa múltiples detecciones de una vez para mayor eficiencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTnNlKaF8DBF"
      },
      "outputs": [],
      "source": [
        "# dino + sam\n",
        "\n",
        "def segment_objects(image, prompt, batch_mode=True):\n",
        "    \"\"\"Segment objects using DINO + SAM pipeline\"\"\"\n",
        "    w, h = image.size\n",
        "    scale = min(768 / max(h, w), 1.0)\n",
        "    new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "    inputs = dino_processor(images=image, text=prompt, return_tensors=\"pt\", do_resize=True, size={\"height\": new_h, \"width\": new_w}).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = dino_model(**inputs)\n",
        "\n",
        "    results = dino_processor.post_process_grounded_object_detection( outputs, inputs.input_ids, threshold=0.5, text_threshold=0.6, target_sizes=[(h, w)])\n",
        "\n",
        "    boxes = results[0]['boxes'].cpu().numpy().tolist()\n",
        "\n",
        "    final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "    if not batch_mode:\n",
        "        for box in boxes: \n",
        "            sam_inputs = sam_processor(images=image, input_boxes=[[box]], return_tensors=\"pt\").to(device) # inferencias una a una\n",
        "\n",
        "            with torch.no_grad():\n",
        "                sam_outputs = sam_model(**sam_inputs)\n",
        "\n",
        "            masks = sam_processor.post_process_masks(sam_outputs.pred_masks, sam_inputs[\"original_sizes\"], sam_inputs[\"reshaped_input_sizes\"])\n",
        "            best_mask = masks[0][0][sam_outputs.iou_scores.argmax()].cpu().numpy()\n",
        "            final_mask += best_mask\n",
        "\n",
        "    elif batch_mode:\n",
        "        if len(boxes) > 0:\n",
        "            sam_inputs = sam_processor(images=image, input_boxes=[boxes], return_tensors=\"pt\").to(device) #inferencia en batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                sam_outputs = sam_model(**sam_inputs)\n",
        "\n",
        "            masks = sam_processor.post_process_masks(sam_outputs.pred_masks, sam_inputs[\"original_sizes\"], sam_inputs[\"reshaped_input_sizes\"])\n",
        "            iou_scores = sam_outputs.iou_scores.cpu().numpy()\n",
        "            final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "            for i in range(len(boxes)):\n",
        "                detection_masks = masks[0][i].cpu().numpy()\n",
        "                best_idx = iou_scores[0][i].argmax()\n",
        "                final_mask += detection_masks[best_idx]\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    final_mask = np.clip(final_mask, 0, 1)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis(\"off\")\n",
        "    axes[1].imshow(final_mask, cmap=\"gray\")\n",
        "    axes[1].set_title(\"Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "    masked_img = np.array(image) * final_mask[:,:,None]\n",
        "    axes[2].imshow(masked_img.astype(np.uint8))\n",
        "    axes[2].set_title(\"Masked Image\")\n",
        "    axes[2].axis(\"off\")\n",
        "    inverted_mask = 1 - final_mask\n",
        "    removed_img = np.array(image) * inverted_mask[:,:,None]\n",
        "    axes[3].imshow(removed_img.astype(np.uint8))\n",
        "    axes[3].set_title(\"Removed Mask\")\n",
        "    axes[3].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return final_mask\n",
        "\n",
        "mask = segment_objects(image_person_cars, [\"cars\", \"woman\"], batch_mode=False) #True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caso Práctico: Control de Calidad en Línea de Producción\n",
        "\n",
        "Segmentación de plátanos para clasificación automática por tamaño. El sistema mide el área de cada plátano detectado y los clasifica como OK/NOK según un umbral predefinido, simulando un proceso de control de calidad industrial.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "bananas_mask = segment_objects(image_bananas, [\"banana\"])\n",
        "threshold_area = 9000  \n",
        "\n",
        "img_array = np.array(image_bananas)\n",
        "result = np.zeros_like(img_array)\n",
        "result[bananas_mask > 0] = img_array[bananas_mask > 0]\n",
        "\n",
        "contours, _ = cv2.findContours(bananas_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "ok_count = nok_count = 0\n",
        "\n",
        "for contour in contours:\n",
        "    area = cv2.contourArea(contour)\n",
        "    if area > 100:\n",
        "        is_ok = area >= threshold_area\n",
        "        color = (0, 255, 0) if is_ok else (255, 0, 0)\n",
        "        if is_ok:\n",
        "            ok_count += 1\n",
        "        else:\n",
        "            nok_count += 1\n",
        "        \n",
        "        cv2.drawContours(result, [contour], -1, color, 3)\n",
        "        \n",
        "        M = cv2.moments(contour)\n",
        "        if M[\"m00\"] != 0:\n",
        "            center_x = int(M[\"m10\"] / M[\"m00\"])\n",
        "            lowest_y = max(contour[:, 0, 1])\n",
        "            cv2.putText(result, f\"Area:{int(area)}\", (center_x-60, lowest_y + 20), \n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(result)\n",
        "plt.title(f\"Bananas area > {threshold_area} mm2 - OK: {ok_count}, NOK: {nok_count}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
