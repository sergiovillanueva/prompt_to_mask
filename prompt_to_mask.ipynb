{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YauhUYCY8DBD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.6.0+cu124, using device: cuda\n",
            "transformers version: 4.52.4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import SamProcessor, SamModel\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "# from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import transformers\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"torch version: {torch.__version__}, using device: {device}\")\n",
        "print(f\"transformers version: {transformers.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DyX6Bf-s8DBE"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/image.jpg\"\n",
        "image = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yb3Xir5k8DBF"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7117cfd60edf40a8bf4368f568bae373",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94f1c5c9ca4648a393748012a7aed2d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d871af5600534c76b46f6c1523ab95ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ad5f777696947a9b619fed555d405e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "827b805450054c18aee6cc489bb4fc09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f92f7f4798174d79a90bba5b9876678b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6bc87f9b17140eab4f0bc51e4aea2df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e29266e09f5b4158946e806b5bdfb685",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# models\n",
        "dino_processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
        "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-base\").to(device)\n",
        "\n",
        "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-large\")\n",
        "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-large\").to(device)\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "qwen_model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=torch.float16, device_map=\"auto\")\n",
        "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
        "\n",
        "# blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "# blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6WTOtOb8DBF"
      },
      "outputs": [],
      "source": [
        "# dino\n",
        "# Ajustar tamaño para DINO\n",
        "w, h = image.size\n",
        "scale = min(768 / max(h, w), 1.0)\n",
        "new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "\n",
        "prompt = [\"grey cars\", \"person\"]\n",
        "\n",
        "# Aplicar Grounding DINO - procesamiento directo\n",
        "inputs = dino_processor(\n",
        "    images=image,\n",
        "    text=prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    do_resize=True,\n",
        "    size={\"height\": new_h, \"width\": new_w}\n",
        ").to(device)\n",
        "\n",
        "# Ejecutar DINO para detección\n",
        "with torch.no_grad():\n",
        "    outputs = dino_model(**inputs)\n",
        "\n",
        "results = dino_processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs.input_ids,\n",
        "            threshold=0.5,\n",
        "            text_threshold=0.5,\n",
        "            target_sizes=[(h, w)])\n",
        "\n",
        "boxes = results[0]['boxes']\n",
        "labels = results[0]['labels']\n",
        "scores = results[0]['scores']\n",
        "\n",
        "# Mostrar resultados\n",
        "for box, label, score in zip(boxes, labels, scores):\n",
        "    print(f\"Box: {box}, Label: {label}, Score: {score}\")\n",
        "\n",
        "\n",
        "img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "for box, label, score in zip(boxes, labels, scores):\n",
        "    x0, y0, x1, y1 = map(int, box)\n",
        "    cv2.rectangle(img_cv, (x0, y0), (x1, y1), (0,0,255), 2)\n",
        "    txt = f\"{label} {score:.2f}\"\n",
        "    cv2.putText(img_cv, txt, (x0, y0-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIoARLaL8DBF"
      },
      "outputs": [],
      "source": [
        "# sam\n",
        "box = [748, 274, 1021, 405]\n",
        "\n",
        "inputs = sam_processor(images=image, input_boxes=[[box]], return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = sam_model(**inputs)\n",
        "\n",
        "masks = sam_processor.post_process_masks(\n",
        "    outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"]\n",
        ")\n",
        "\n",
        "best_mask = masks[0][0][outputs.iou_scores.argmax()].cpu().numpy()\n",
        "\n",
        "print(f\"Mask shape: {best_mask.shape}\")\n",
        "\n",
        "\n",
        "plt.imshow(best_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTnNlKaF8DBF"
      },
      "outputs": [],
      "source": [
        "# dino + sam\n",
        "w, h = image.size\n",
        "scale = min(768 / max(h, w), 1.0)\n",
        "new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "prompt = [\"car\", \"person\"]\n",
        "\n",
        "inputs = dino_processor(\n",
        "    images=image,\n",
        "    text=prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    do_resize=True,\n",
        "    size={\"height\": new_h, \"width\": new_w}\n",
        ").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = dino_model(**inputs)\n",
        "\n",
        "results = dino_processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs.input_ids,\n",
        "            threshold=0.5,\n",
        "            text_threshold=0.5,\n",
        "            target_sizes=[(h, w)])\n",
        "\n",
        "boxes = results[0]['boxes'].cpu().numpy().tolist()\n",
        "scores = results[0]['scores'].cpu().numpy().tolist()\n",
        "labels = results[0]['labels']\n",
        "\n",
        "\n",
        "final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "\n",
        "for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):\n",
        "    print(f\"Processing {label} (score: {score:.3f})\")\n",
        "    sam_inputs = sam_processor(images=image, input_boxes=[[box]], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sam_outputs = sam_model(**sam_inputs)\n",
        "\n",
        "    masks = sam_processor.post_process_masks(\n",
        "        sam_outputs.pred_masks, sam_inputs[\"original_sizes\"], sam_inputs[\"reshaped_input_sizes\"]\n",
        "    )\n",
        "\n",
        "    best_mask = masks[0][0][sam_outputs.iou_scores.argmax()].cpu().numpy()\n",
        "\n",
        "    final_mask += best_mask\n",
        "\n",
        "final_mask = np.clip(final_mask, 0, 1)\n",
        "\n",
        "print(f\"Final mask shape: {final_mask.shape}\")\n",
        "\n",
        "plt.imshow(final_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDhCnqmW8DBF"
      },
      "outputs": [],
      "source": [
        "# dino + sam batch\n",
        "w, h = image.size\n",
        "scale = min(768 / max(h, w), 1.0)\n",
        "new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "prompt = [\"car\", \"person\"]\n",
        "\n",
        "inputs = dino_processor(\n",
        "    images=image,\n",
        "    text=prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    do_resize=True,\n",
        "    size={\"height\": new_h, \"width\": new_w}\n",
        ").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = dino_model(**inputs)\n",
        "\n",
        "results = dino_processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs.input_ids,\n",
        "            threshold=0.5,\n",
        "            text_threshold=0.5,\n",
        "            target_sizes=[(h, w)])\n",
        "\n",
        "boxes = results[0]['boxes'].cpu().numpy().tolist()\n",
        "scores = results[0]['scores'].cpu().numpy().tolist()\n",
        "labels = results[0]['labels']\n",
        "\n",
        "\n",
        "if len(boxes) > 0:\n",
        "    sam_inputs = sam_processor(images=image, input_boxes=[boxes], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sam_outputs = sam_model(**sam_inputs)\n",
        "\n",
        "    masks = sam_processor.post_process_masks(\n",
        "        sam_outputs.pred_masks, sam_inputs[\"original_sizes\"], sam_inputs[\"reshaped_input_sizes\"]\n",
        "    )\n",
        "\n",
        "    iou_scores = sam_outputs.iou_scores.cpu().numpy()\n",
        "    final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        detection_masks = masks[0][i].cpu().numpy()\n",
        "        best_idx = iou_scores[0][i].argmax()\n",
        "        final_mask += detection_masks[best_idx]\n",
        "\n",
        "    final_mask = np.clip(final_mask, 0, 1)\n",
        "    print(f\"Processed {len(boxes)} detections in single batch\")\n",
        "else:\n",
        "    final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "plt.imshow(final_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWJCKTTX8DBF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'a photo of a street with a person' -> Probability: 0.91\n",
            "'a photo of a dog' -> Probability: 0.00\n",
            "'a photo of several cars but without any person on it' -> Probability: 0.09\n"
          ]
        }
      ],
      "source": [
        "# clip\n",
        "descriptions = [\"a photo of a street with a person\", \"a photo of a dog\", \"a photo of several cars but without any person on it\"]\n",
        "\n",
        "inputs = clip_processor(text=descriptions, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "outputs = clip_model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image\n",
        "probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "for desc, prob in zip(descriptions, probs[0].tolist()):\n",
        "    print(f\"'{desc}' -> Probability: {prob:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a photography of a woman crossing a street\n"
          ]
        }
      ],
      "source": [
        "# blip\n",
        "# inputs = blip_processor(image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "# out = blip_model.generate(**inputs)\n",
        "# print(blip_processor.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How many persons are there?\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'qwen_processor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      6\u001b[39m                 {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: image},\n\u001b[32m      7\u001b[39m                 {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: question}]}]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m text = \u001b[43mqwen_processor\u001b[49m.apply_chat_template(messages, tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m inputs = qwen_processor(\n\u001b[32m     13\u001b[39m     text=[text],\n\u001b[32m     14\u001b[39m     images=[image],\n\u001b[32m     15\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m ).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
            "\u001b[31mNameError\u001b[39m: name 'qwen_processor' is not defined"
          ]
        }
      ],
      "source": [
        "question = \"How many persons are there?\"\n",
        "print(f\"Question: {question}\")\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": [\n",
        "    {\"type\": \"image\", \"image\": image},\n",
        "    {\"type\": \"text\", \"text\": question}\n",
        "]}]\n",
        "\n",
        "text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = qwen_processor(text=[text], images=[image], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = qwen_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
        "\n",
        "generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
        "answer = qwen_processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "print(f\"Answer: {answer}\")\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
