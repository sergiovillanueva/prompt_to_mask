{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YauhUYCY8DBD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import SamProcessor, SamModel\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import transformers\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"torch version: {torch.__version__}, using device: {device}\")\n",
        "print(f\"transformers version: {transformers.__version__}\")\n",
        "print(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyX6Bf-s8DBE"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/sergiovillanueva/prompt_to_mask/raw/master/assets/image.jpg\"\n",
        "image = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb3Xir5k8DBF"
      },
      "outputs": [],
      "source": [
        "dino_processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-base\")\n",
        "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-base\").to(device)\n",
        "\n",
        "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-large\")\n",
        "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-large\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6WTOtOb8DBF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ajustar tamaño para DINO\n",
        "w, h = image.size\n",
        "scale = min(768 / max(h, w), 1.0)\n",
        "new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "\n",
        "prompt = [\"grey cars\", \"person\"]\n",
        "\n",
        "# Aplicar Grounding DINO - procesamiento directo\n",
        "inputs = dino_processor(\n",
        "    images=image,\n",
        "    text=prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    do_resize=True,\n",
        "    size={\"height\": new_h, \"width\": new_w}\n",
        ").to(device)\n",
        "\n",
        "# Ejecutar DINO para detección\n",
        "with torch.no_grad():\n",
        "    outputs = dino_model(**inputs)\n",
        "\n",
        "results = dino_processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs.input_ids,\n",
        "            threshold=0.5,\n",
        "            text_threshold=0.5,\n",
        "            target_sizes=[(h, w)])\n",
        "\n",
        "boxes = results[0]['boxes']\n",
        "labels = results[0]['labels']\n",
        "scores = results[0]['scores']\n",
        "\n",
        "# Mostrar resultados\n",
        "for box, label, score in zip(boxes, labels, scores):\n",
        "    print(f\"Box: {box}, Label: {label}, Score: {score}\")\n",
        "\n",
        "\n",
        "img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "for box, label, score in zip(boxes, labels, scores):\n",
        "    x0, y0, x1, y1 = map(int, box)\n",
        "    cv2.rectangle(img_cv, (x0, y0), (x1, y1), (0,0,255), 2)\n",
        "    txt = f\"{label} {score:.2f}\"\n",
        "    cv2.putText(img_cv, txt, (x0, y0-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIoARLaL8DBF"
      },
      "outputs": [],
      "source": [
        "\n",
        "box = [748, 274, 1021, 405]\n",
        "\n",
        "inputs = sam_processor(images=image, input_boxes=[[box]], return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = sam_model(**inputs)\n",
        "\n",
        "masks = sam_processor.post_process_masks(\n",
        "    outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"]\n",
        ")\n",
        "\n",
        "best_mask = masks[0][0][outputs.iou_scores.argmax()].cpu().numpy()\n",
        "\n",
        "print(f\"Mask shape: {best_mask.shape}\")\n",
        "\n",
        "\n",
        "plt.imshow(best_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTnNlKaF8DBF"
      },
      "outputs": [],
      "source": [
        "\n",
        "w, h = image.size\n",
        "scale = min(768 / max(h, w), 1.0)\n",
        "new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "prompt = [\"car\", \"person\"]\n",
        "\n",
        "inputs = dino_processor(\n",
        "    images=image,\n",
        "    text=prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    do_resize=True,\n",
        "    size={\"height\": new_h, \"width\": new_w}\n",
        ").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = dino_model(**inputs)\n",
        "\n",
        "results = dino_processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs.input_ids,\n",
        "            threshold=0.5,\n",
        "            text_threshold=0.5,\n",
        "            target_sizes=[(h, w)])\n",
        "\n",
        "boxes = results[0]['boxes'].cpu().numpy().tolist()\n",
        "scores = results[0]['scores'].cpu().numpy().tolist()\n",
        "labels = results[0]['labels']\n",
        "\n",
        "\n",
        "final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "\n",
        "for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):\n",
        "    print(f\"Processing {label} (score: {score:.3f})\")\n",
        "    sam_inputs = sam_processor(images=image, input_boxes=[[box]], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sam_outputs = sam_model(**sam_inputs)\n",
        "\n",
        "    masks = sam_processor.post_process_masks(\n",
        "        sam_outputs.pred_masks, sam_inputs[\"original_sizes\"], sam_inputs[\"reshaped_input_sizes\"]\n",
        "    )\n",
        "\n",
        "    best_mask = masks[0][0][sam_outputs.iou_scores.argmax()].cpu().numpy()\n",
        "\n",
        "    final_mask += best_mask\n",
        "\n",
        "final_mask = np.clip(final_mask, 0, 1)\n",
        "\n",
        "print(f\"Final mask shape: {final_mask.shape}\")\n",
        "\n",
        "plt.imshow(final_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDhCnqmW8DBF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "w, h = image.size\n",
        "scale = min(768 / max(h, w), 1.0)\n",
        "new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "prompt = [\"car\", \"person\"]\n",
        "\n",
        "inputs = dino_processor(\n",
        "    images=image,\n",
        "    text=prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    do_resize=True,\n",
        "    size={\"height\": new_h, \"width\": new_w}\n",
        ").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = dino_model(**inputs)\n",
        "\n",
        "results = dino_processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs.input_ids,\n",
        "            threshold=0.5,\n",
        "            text_threshold=0.5,\n",
        "            target_sizes=[(h, w)])\n",
        "\n",
        "boxes = results[0]['boxes'].cpu().numpy().tolist()\n",
        "scores = results[0]['scores'].cpu().numpy().tolist()\n",
        "labels = results[0]['labels']\n",
        "\n",
        "\n",
        "if len(boxes) > 0:\n",
        "    sam_inputs = sam_processor(images=image, input_boxes=[boxes], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sam_outputs = sam_model(**sam_inputs)\n",
        "\n",
        "    masks = sam_processor.post_process_masks(\n",
        "        sam_outputs.pred_masks, sam_inputs[\"original_sizes\"], sam_inputs[\"reshaped_input_sizes\"]\n",
        "    )\n",
        "\n",
        "    iou_scores = sam_outputs.iou_scores.cpu().numpy()\n",
        "    final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        detection_masks = masks[0][i].cpu().numpy()\n",
        "        best_idx = iou_scores[0][i].argmax()\n",
        "        final_mask += detection_masks[best_idx]\n",
        "\n",
        "    final_mask = np.clip(final_mask, 0, 1)\n",
        "    print(f\"Processed {len(boxes)} detections in single batch\")\n",
        "else:\n",
        "    final_mask = np.zeros((h, w), dtype=np.float32)\n",
        "\n",
        "plt.imshow(final_mask, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWJCKTTX8DBF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}